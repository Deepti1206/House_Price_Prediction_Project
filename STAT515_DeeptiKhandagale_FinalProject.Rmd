---
title: "House Price Prediction"
author: "Deepti Vijay Khandagale, Awantika Shah"
date: "5/7/2022"
output:
  word_document: default
  pdf_document: default
---
# PROJECT OBJECTIVE
The objective of this project is to predict the final price of the house using different statistical models.

# BACKGROUND
In this project, we are using the "Ames Housing dataset" from the kaggle competition which is originally prepared by Dean De Cock. The data contains the following things:
- train.csv : the training set\
- test.csv : the test set\
- data_description.txt : full description of each column (columns edited)

# DATASET OVERVIEW
The aim is to use statistical methods for analysis, find out the important features and perform regression techniques to build a prediction model. The data contains 81 variables and 1,460 observations describing the different parameters of the residential homes in Ames, Iowa. With the train dataset, we intend to fit a linear model which will predict the final price of the house. 

# METHODOLOGY
In order to build a reliable model, we have performed the following steps: 

1. *Data Quality and Cleaning* : It is necessary that our data should be clean, hence we did some exploratory data analysis to check the data quality and performed data cleaning. 
2. *Feature Engineering*: We have approx 81 columns to predict the house price which has mixture of numerical and categorical data. There is lot of opportunities to perform feature engineering to produce some strong features that will help in building a robust model. 
3. *Training Model* : After the data cleaning and feature engineering, we will be training and tuning various linear regression models and will diagnostic the models. 
4. *Prediction and Model Comparison* : Once we train our models we will compare their performances by testing it on the Test data and compare the MSE, MAE and RSME metrics.

**Loading the dataset**\
```{r setup, include=FALSE, results='hide'}
knitr::opts_chunk$set(echo = TRUE)
setwd(getwd())
source("DataCleaning.R")
library(dplyr)
library(psych)
library(Hmisc)
library(ggplot2)
library(mice)
library(VIM)
library(missForest)
library(comprehenr)
library(ggpubr)
library(glmnet)
library(MASS)
library(randomForest)
library(caret)
library(Metrics)

train_house <- read.csv("data/train.csv")
```
The train data has 1460 observations and 81 variables. In this project, our response variable is "SalePrice" which we want to predict and we'll be regressing the response variable **"SalePrice"** w.r.t to the predictors i.e the rest of the variables.

## Data Quality and Cleaning
The first step for the data cleaning is to check for the missing values. Missing values can be nightmare for any analysis, hence we will be finding the missing values in the dataset.

```{r houseprice , fig.width=10,fig.height=5 , echo= FALSE }
na_cols <- data.frame(colSums(is.na(train_house)))
na_cols <- cbind(Col_name = rownames(na_cols), na_cols)
colnames(na_cols ) <- c( 'Col_name' , 'Missing_Count')
na_cols <- na_cols[na_cols$Missing_Count != 0 , ]
na_cols <- na_cols[order(- na_cols$Missing_Count) , ]
rownames(na_cols) <- 1:nrow(na_cols)
na_cols$Total <- 1460
na_cols$perc_missing <- round(na_cols$Missing_Count / na_cols$Total , 3)
#head(na_cols)
ggplot(na_cols, aes(x=perc_missing, y=reorder(Col_name,perc_missing))) + geom_bar(stat = "identity")
```
Above we can see in the graph that there are 19 variables which have missing values and 5 of them have more than ~ 50% of missing values. We decided to drop all the columns which has more than 40% of missing data (eliminating 5 variables : PoolQC, MiscFeature, Alley, Fence, FireplaceQu ). For the rest of the variables we have performed data imputation as mentioned below. 

1. *Numerical Variables* : All the numerical variables which had missing values were replaced by the median of the variable as some of the variables had skewed distribution.
2. *Categorical Variables* : All the Categorical variables which had missing values were replaced by the Mode of the variable.
3. *Miscellaneous Variables* : There were some variables which had meaning for the missing values, example : Categorical columns related to Basement had missing values which is associated with the No Basement. Similarly we had missing values in the Garage columns which means there is no garage for that observation. 
4. There are columns like **Street** and **Utilities**  which had only 2 categories and were highly skewed. We decided to drop these two columns as they hold no great value here. Below is the Bar plot of these two columns. 
```{r Street , fig.width=10,fig.height=2 , echo=FALSE }

Street <- ggplot(train_house , aes(x = Street)) + geom_bar() 
Utilities <- ggplot(train_house , aes(x = Utilities)) + geom_bar() 
ggarrange(Street , Utilities
          ,  ncol = 2, nrow = 1)

```

## Feature Engineering\
Feature engineering is one of the most important steps in any Machine Learning process. We can improve the performance of any model by creative feature engineering. As we know the variables are treated differently depending on their datatype we must make sure to prepare these columns for training our models. Hence we will first tackle all the categorical variables first to see if there is any opportunity for feature engineering. We will be categorizing all the categorical variables based on their type which are as follows :

**Year of Construction**:
There are 4 variables with years (yearBuilt, yearRemodled, GarageYrBlt and YrSold) each ploted against the Sale price. One can notice that the sale price are increasing with the years for the yearBuilt, yearRemodled, GarageYrBlt columns and there is no differnce in the Sale price for different YrSold. Hence we will be dropping YrSold and it's associated MoSold column as it shows no impact on the final Saleprice. As the Sale price is increasing with the years in other three variables, it is natural to keep these variables as numerical. We can expect some collinearity between these variable and should consider only one out of all, but for now we will keep all of them and let our models do the magic. 

```{r YearBuild , fig.width=10,fig.height=4 , echo=FALSE}

train_house$YearBuilt <- as.factor(train_house$YearBuilt)
train_house$GarageYrBlt <- as.factor(train_house$GarageYrBlt)
train_house$YearRemodAdd <- as.factor(train_house$YearRemodAdd)
train_house$YrSold <- as.factor(train_house$YrSold)
train_house$SalePrice <- train_house$SalePrice/1000000


yearBuilt <- ggplot(train_house , aes(x = YearBuilt, y = SalePrice)) + geom_boxplot()+ theme(axis.text.x = element_text(angle = 90)) 
yearRemodled <- ggplot(train_house, aes(x = YearRemodAdd, y = SalePrice))+ geom_boxplot()+ theme(axis.text.x = element_text(angle = 90))
GarageYrBlt <- ggplot(train_house , aes(x = GarageYrBlt, y = SalePrice)) + geom_boxplot()+ theme(axis.text.x = element_text(angle = 90))
YrSold <- ggplot(train_house , aes(x = YrSold, y = SalePrice)) + geom_boxplot()+ theme(axis.text.x = element_text(angle = 90))

ggarrange(yearBuilt, yearRemodled , GarageYrBlt , YrSold ,  ncol = 2, nrow = 2)

```
**Basement**:
In this category, we included all the variables related to basements like BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2 and BsmtQual. We have created the boxplot of sale Price against all these variables and we observed that the sale price is affected by BsmtCond, BsmtExposure and BsmtQual, i.e better the quality better the price. Hence it is safe to assign ordinal values to these categorical values which shall be treated as numerical value in our model. This way we will be able to reduce dimensions in our model to some extent. BsmtfinType1 and BsmtfinType1 doesn't affect much so we will leave it as it is.

```{r Basement , fig.width=10,fig.height=4 ,  echo= FALSE}
train_house <- read.csv("data/train.csv")
train_house$SalePrice <- train_house$SalePrice/1000000

BsmtCond <- ggplot(subset(train_house , !is.na(train_house$BsmtCond)) 
       , aes(x = reorder(BsmtCond , SalePrice) , y = SalePrice)) +
  geom_boxplot() + scale_x_discrete("BsmtCond") + ylab("SalePrice in Million")

BsmtExposure <- ggplot(subset(train_house , !is.na(train_house$BsmtExposure)) 
       , aes(x = reorder(BsmtExposure , SalePrice) , y = SalePrice)) +
  geom_boxplot() + scale_x_discrete("BsmtExposure")+ ylab("SalePrice in Million")

BsmtFinType1 <- ggplot(subset(train_house , !is.na(train_house$BsmtFinType1)) 
       , aes(x = reorder(BsmtFinType1 , SalePrice) , y = SalePrice)) + 
  geom_boxplot() + scale_x_discrete("BsmtFinType1")+ ylab("SalePrice in Million")

BsmtFinType2 <- ggplot(subset(train_house , !is.na(train_house$BsmtFinType2)) 
       , aes(x = reorder(BsmtFinType2 , SalePrice) , y = SalePrice)) + 
  geom_boxplot() + scale_x_discrete("BsmtFinType2")+ ylab("SalePrice in Million")

BsmtQual <- ggplot(subset(train_house , !is.na(train_house$BsmtQual)) 
       , aes(x = reorder(BsmtQual , SalePrice) , y = SalePrice)) + 
  geom_boxplot() + scale_x_discrete("BsmtQual")+ ylab("SalePrice in Million")

ggarrange(BsmtCond, BsmtExposure , BsmtFinType1 , BsmtFinType2, BsmtQual
          ,  ncol = 3, nrow = 2)
```
**Garage & Exterior**:
Just like in Basement, we did similar exercise for the Garage and Exterior,we found some relation with the GarageQual, GarageCond and GarageFinish; ExterCond and ExterQual. Hence it is safe to assign ordinal values to these categorical values which shall be treated as numerical value in our model. For others, it doesn't affect much so we will leave it as it is.
```{r Garage6, fig.width=10,fig.height=4 , echo= FALSE}

GarageCond <- ggplot(subset(train_house , !is.na(train_house$GarageCond)) 
       , aes(x = reorder(GarageCond , SalePrice) , y = SalePrice)) +
  geom_boxplot() + scale_x_discrete("GarageCond") + ylab("SalePrice in Million")

GarageFinish <- ggplot(subset(train_house , !is.na(train_house$GarageFinish)) 
       , aes(x = reorder(GarageFinish , SalePrice) , y = SalePrice)) +
  geom_boxplot() + scale_x_discrete("GarageFinish") + ylab("SalePrice in Million")

GarageQual <- ggplot(subset(train_house , !is.na(train_house$GarageQual)) 
       , aes(x = reorder(GarageQual , SalePrice) , y = SalePrice)) + 
  geom_boxplot() + scale_x_discrete("GarageQual") + ylab("SalePrice in Million")

GarageType <- ggplot(subset(train_house , !is.na(train_house$GarageType)) 
       , aes(x = reorder(GarageType , SalePrice) , y = SalePrice)) + 
  geom_boxplot() + scale_x_discrete("GarageType") + ylab("SalePrice in Million")

ExterCond <- ggplot(subset(train_house , !is.na(train_house$ExterCond)) 
       , aes(x = reorder(ExterCond , SalePrice) , y = SalePrice)) +
  geom_boxplot() + scale_x_discrete("ExterCond") + ylab("SalePrice in Million")

ExterQual <- ggplot(subset(train_house , !is.na(train_house$ExterQual)) 
       , aes(x = reorder(ExterQual , SalePrice) , y = SalePrice)) + 
  geom_boxplot() + scale_x_discrete("ExterQual") + ylab("SalePrice in Million")


ggarrange( GarageCond , GarageFinish , GarageQual, GarageType , ExterQual , ExterCond
          ,  ncol = 3, nrow = 2)

```
**Construction**:
For the construction, in the variables like Foundation, LandContour, LandSlope, NldgType, MasvnrType, Lot Shape, KitchenQual, RoofStyl and RoofMatl ,there is no ordinal data and no pattern was found. So we are keeping it as Categorical

**Appliance**:
In appliance, Heating and Electrical showed some pattern. Hence we assign ordinal values to these categorical values which shall be treated as numerical value. Rest we will leave as it is.

```{r Appliance , fig.width=10,fig.height=3 , echo= FALSE}

CentralAir <- ggplot(subset(train_house , !is.na(train_house$CentralAir)) 
       , aes(x = reorder(CentralAir , SalePrice) , y = SalePrice)) +
  geom_boxplot() + scale_x_discrete("CentralAir") + theme(axis.text.x = element_text(angle = 90))+ ylab("SalePrice in Million")

Electrical <- ggplot(subset(train_house , !is.na(train_house$Electrical)) 
       , aes(x = reorder(Electrical , SalePrice) , y = SalePrice)) +
  geom_boxplot() + scale_x_discrete("Electrical") + theme(axis.text.x = element_text(angle = 90))+ ylab("SalePrice in Million")

Functional <- ggplot(subset(train_house , !is.na(train_house$Functional)) 
       , aes(x = reorder(Functional , SalePrice) , y = SalePrice)) +
  geom_boxplot() + scale_x_discrete("Functional") + theme(axis.text.x = element_text(angle = 90))+ ylab("SalePrice in Million")

Heating <- ggplot(subset(train_house , !is.na(train_house$Heating)) 
       , aes(x = reorder(Heating , SalePrice) , y = SalePrice)) +
  geom_boxplot() + scale_x_discrete("Heating") + theme(axis.text.x = element_text(angle = 90))+ ylab("SalePrice in Million")

ggarrange(   Electrical  , Heating,  ncol = 2, nrow = 1)
```
**Miscellaneous**:
Other Columns on Miscellaneous include HeatingQC, HouseStyle, LotConfig, SaleCondition, SaleType, Functional and CentralAc doesn't show any pattern, hence we treat it as categorical only.

**SalePrice**:
Let us check the linearity assumption by plotting a histogram of our target variable sale price. One the left graph we can see that the sale price distribution is rightly skewed which means that the assumption of linearity may not be true here. In order to mitigate this we can take the natural log of the sale price and check the distribution. On the right we can see that the log distribution looks normally distributed, hence we shall be training our model using the Log of the target variable. 

```{r SalePrice, fig.width=10,fig.height=3  , echo=FALSE}

df_hist <- data.frame(train_house$SalePrice)
colnames(df_hist) <- c("SalePrice")
df_hist$SalePrice_log <- log(df_hist$SalePrice)

norm_hist <- ggplot(df_hist , aes(x = SalePrice)) + geom_histogram(bins = 50)
logHist <-   ggplot(df_hist , aes(x = SalePrice_log)) + geom_histogram(bins = 50)

ggarrange(  norm_hist , logHist,  ncol = 2, nrow = 1)

```
# TRAINING MODEL
```{r DataCleaning, echo= FALSE}

train_house <- read.csv("data/train.csv")
train_house <- data_cleaning(train_house , thres = 0.4)
train_house <- feature_eng(train_house)

test_house <- read.csv("data/test.csv")
test_house <- TestDataPrep(test_house)

test_Saleprice <- read.csv("data/sample_submission.csv")
```
Now that we have done the heavy lifting of data cleaning and feature engineering, it's time to train our model and get some prediction. For this project we will be focusing on the Linear regression techniques we have learned in the Class and shall be implementing 4 type of linear regression which are as follows:

## Multiple Linear Regression (with Cross Validation)
```{r LinearModel , echo= TRUE ,  results='hide' , warning=FALSE}
# knitr::opts_chunk$set(warning = FALSE, message = FALSE)
custom <- trainControl(method = "repeatedcv",number = 10, repeats = 5, verboseIter = F)
lm <- train(log(SalePrice)~. , train_house,  method = 'lm',  trControl = custom)
```

```{r LinearModelsummary, fig.width=10,fig.height=4 , echo= TRUE, results='hide'}
summary(lm)
lm$results
lm
```
```{r LinearModelGraph, fig.width=10,fig.height=4 , echo= TRUE}
par(mfrow=c(2,2))
plot(lm$finalModel)
```
In multiple linear regression, the output of the data is interpreted using the diagnostics plot:
-Residual v/s fitted graph doesn't show any pattern in the graph, which means that there is no non-linear relationship between the predictor and outcome variables 
- Normal Q-Q:The residuals are normally distributed. The points forming but there are few outliers at the lower side
- Scale-Location: This graphs checks the assumption of homoscedasticity and as seen the residuals are scattered randomly which satisfies assumption
- Residual vs Leverage: This graphs help us to identify the influential outliers that might affect the analysis and here outside Cook's distance (red dotted) we find two outliers.

## Ridge Linear Regression (with Cross Validation)

```{r Ridge5 , fig.width=10,fig.height=4, echo= TRUE ,  warning=FALSE}
set.seed(1234)
custom <- trainControl(method = "repeatedcv", number = 10, repeats = 5, verboseIter = F)
ridge <- train(log(SalePrice)~. ,train_house,method = 'glmnet',tuneGrid = expand.grid(alpha = 0,lambda = seq(0.0001,1,length = 5)),trControl = custom)
ridge
```
```{r Ridgeplots , fig.width=10,fig.height=4, echo= TRUE ,  warning=FALSE, results='hide'}
ridge_plot <- plot(ridge)
ridge_lamda <- plot(ridge$finalModel , xvar = 'lambda' , label = T)
ridge_dev <- plot(ridge$finalModel , xvar = 'dev' , label = T)
ridge_30 <- plot(varImp(ridge , scale = F) , top=30)
ridge_30
```
In Ridge linear regression, it penalize using Lamda on the co-efficient, if lamda is increasing, the co-efficients tends towards zero. We find the best lamda, which gives us the important variables for our model. 
So here, the best lamda is 0.25 and it provides the top 30 important variables as Condition2PosN, Condition2PosA, FunctionalSev, FunctionalMaj2, RoofMatWdshngl, ExtFirstBrkComm and so on.

## Lasso Linear Regression (with Cross Validation)

```{r Lasso1 , echo= TRUE , fig.width=10,fig.height=4, warning=FALSE, results='hide'}
set.seed(1234)
custom <- trainControl(method = "repeatedcv",number = 10, repeats = 5, verboseIter = T)

lasso <- train(log(SalePrice)~. ,train_house, method = 'glmnet',tuneGrid = expand.grid(alpha = 1, lambda = seq(0.0001,0.2,length = 5)),trControl = custom)
```
```{r lassoplot, fig.width=10,fig.height=4, results='hide', echo=TRUE}
par(mfrow=c(2,2))
plot(lasso)
plot(lasso$finalModel , xvar = 'lambda' , label = T)
plot(lasso$finalModel , xvar = 'dev' , label = T)
plot(varImp(lasso , scale = FALSE) , top=30 )

```
In lasso regression, the lamda shrinks coefficient completely to zero and removes the unnecessary predictors. So here, the best lamda is very small 0.05 and it provides the top 30 important variables as . The topmost variables are OverallQual, GarageCars, KitchenQual, MSZoningRM and BsmtQual.

## Random Forest Regressor 
```{r random6, fig.width=10,fig.height=4, echo=TRUE}
set.seed(123)
RF = randomForest(log(train_house$SalePrice)~., data=train_house[-71],
                        mtry=20, ntree=500,importance=TRUE, na.action = na.omit)
RF

head(importance(RF))
varImpPlot(RF)
```
```{r randomtest,echo=FALSE}
levels(test_house$Condition2) <- levels(train_house$Condition2)
levels(test_house$HouseStyle) <- levels(train_house$HouseStyle)
levels(test_house$Exterior1st) <- levels(train_house$Exterior1st)
levels(test_house$Exterior2nd) <- levels(train_house$Exterior2nd)
levels(test_house$RoofMatl) <- levels(train_house$RoofMatl)
levels(test_house$Heating) <- levels(train_house$Heating)
levels(test_house$Electrical) <- levels(train_house$Electrical)

```

In random forest, Mean of squared residuals: 0.01841201 means the prediction error. 88.45% variance is explained by the variables. In the first plot, the higher the increase in MSE, the important is the variable. we see that GrLivArea, Neighborhood, TotalBsmiSF and so on are most important. In second, node purity is depended on the Gini Index and here the important variables are Overall Qual, Neighborhood, GrLivArea etc.

# PREDICTION NAD MODEL COMPARISON

Now we have trained our models on the train data, it is time to test it on the test data. We have done the same level of data quality checking and cleaning as it was applied to train data.

```{r linearpredict, echo=TRUE, warning=FALSE}
pred.lm = predict(lm,newdata=test_house)
rmse(log(test_Saleprice$SalePrice), pred.lm )

pred.ridge = predict(ridge,newdata=test_house)
rmse(log(test_Saleprice$SalePrice), pred.ridge)

pred.lasso = predict(lasso,newdata=test_house)
rmse(log(test_Saleprice$SalePrice), pred.lasso )

pred.RF = predict(RF,newdata=test_house)
rmse(log(test_Saleprice$SalePrice), pred.RF )

```

# CONCLUSION
As per our analysis on the test data, it is seen that Root Mean Square error of Lasso regression is lowest and hence we select the **Lasso Regression** the best fit for our prediction of sale price.The topmost variables are OverallQual, GarageCars, KitchenQual, MSZoningRM and BsmtQual.

# REFERENCES
- House Price Prediction. Retrieved from: https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/overview
- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning: With applications in R.
- R Codes. retrieved from: https://www.rdocumentation.org/
- Output Summary: https://github.com/Deepti1206/House_Price_Prediction_Project.git









